Embedding Models
===

Course Link - [Embedding Models](https://www.deeplearning.ai/short-courses/embedding-models-from-architecture-to-implementation/)

This course goes into the details of the architecture and capabilities of embedding models, which are used in many AI applications to capture the meaning of words and sentences.

You will learn about the evolution of embedding models, from word to sentence embeddings, and build and train a simple dual encoder model. This hands-on approach will help you understand the technical concepts behind embedding models and how to use them effectively.

- Learn about word embedding, sentence embedding, and cross-encoder models; and how they can be used in RAG
- Understand how transformer models, specifically BERT (Bi-directional Encoder Representations from Transformers), are trained and used in semantic search systems
- Gain knowledge of the evolution of sentence embedding and understand how the dual encoder architecture was formed
- Use a contrastive loss to train a dual encoder model, with one encoder trained for questions and another for the responses
- Utilize separate encoders for question and answer in a RAG pipeline and see how it affects the retrieval compared to using a single encoder model.
